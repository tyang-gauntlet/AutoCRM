import { ChatMessage } from '@/types/chat'
import { RunnableSequence } from '@langchain/core/runnables'
import { ChatOpenAI } from '@langchain/openai'
import { HumanMessage, SystemMessage } from '@langchain/core/messages'
import { AgentAction, AgentResponse, RAGContext } from './agent-interfaces'
import { searchKnowledge, formatContext } from './rag'
import { executeToolCall } from './tools'
import { recordKRAMetrics, recordRGQSMetrics } from './metrics'
import { SYSTEM_PROMPT } from './prompts'

// Define our conversation state type
interface ConversationState {
    messages: ChatMessage[]
    context: RAGContext[]
    userId: string
    ticketId: string | null
    currentMessage: string
    needsTicket: boolean
    toolCalls: any[]
    response: string | null
    metrics: {
        kra?: any
        rgqs?: any
    }
    ticketDetails?: {
        title: string
        description: string
        priority: string
    }
    intent?: 'greeting' | 'question' | 'affirmative' | 'unknown'
    isFirstMessage?: boolean
}

// Initialize LLM
const model = new ChatOpenAI({
    modelName: 'gpt-4-turbo-preview',
    temperature: 0.7,
    maxTokens: 2000
})

// Create nodes for our conversation graph
const nodes = {
    // Start by analyzing intent
    analyze_intent: async (state: ConversationState) => {
        // If this is the first message and no previous messages, return initial greeting
        if (state.isFirstMessage && state.messages.length === 0) {
            return {
                ...state,
                intent: 'greeting',
                response: "ðŸ‘‹ Hello! I'm your AutoCRM AI assistant. How can I help you today?"
            }
        }

        // Check for greetings
        const isGreeting = /^(hi|hello|hey|greetings|good\s*(morning|afternoon|evening))$/i.test(state.currentMessage.trim())

        // Check for affirmative responses
        const isAffirmative = /^(yes|yeah|sure|ok|okay|yep|y)$/i.test(state.currentMessage.trim())
        const lastAssistantMessage = state.messages
            .filter(msg => msg.role === 'assistant')
            .pop()
        const wasOfferingTicket = lastAssistantMessage?.content.includes('Would you like me to create a support ticket')

        return {
            ...state,
            intent: isGreeting ? 'greeting'
                : (isAffirmative && wasOfferingTicket) ? 'affirmative'
                    : 'question'
        }
    },

    // Gather context only for questions
    gather_context: async (state: ConversationState) => {
        // Skip context gathering for greetings and if we already have a response
        if (state.intent === 'greeting' || state.response) {
            return state
        }

        const context = await searchKnowledge(state.currentMessage)
        return {
            ...state,
            context,
            metrics: {
                ...state.metrics,
                kra: {
                    query_text: state.currentMessage,
                    retrieved_chunks: context.length,
                    relevant_chunks: context.filter(c => c.similarity > 0.85).length,
                    accuracy: context[0]?.similarity || 0,
                    relevance_score: context.reduce((acc, c) => acc + c.similarity, 0) / (context.length || 1),
                    context_match: context.length ? 1 : 0
                }
            }
        }
    },

    // Analyze ticket need only for non-greetings
    analyze_ticket_need: async (state: ConversationState) => {
        if (state.intent === 'greeting' || state.response) return { ...state, needsTicket: false }

        // For general feature inquiries, don't immediately suggest a ticket
        if (state.currentMessage.toLowerCase().includes('feature') ||
            state.currentMessage.toLowerCase().includes('can you') ||
            state.currentMessage.toLowerCase().includes('what do you')) {
            return { ...state, needsTicket: false }
        }

        if (state.intent === 'affirmative') {
            const lastAssistantMessage = state.messages
                .filter(msg => msg.role === 'assistant')
                .pop()

            // Only create ticket if we explicitly offered one
            if (!lastAssistantMessage?.content.includes('create a support ticket')) {
                return { ...state, needsTicket: false }
            }

            // Find original request
            const originalRequest = state.messages
                .filter(msg => msg.role === 'user')
                .slice(-2)[0]?.content || state.currentMessage

            return {
                ...state,
                needsTicket: true,
                ticketDetails: {
                    title: 'Support Request: ' + originalRequest.slice(0, 100),
                    description: originalRequest,
                    priority: state.context.length ? 'medium' : 'high'
                }
            }
        }

        return { ...state, needsTicket: false }
    },

    // Create ticket if needed
    handle_ticket: async (state: ConversationState) => {
        if (!state.needsTicket || !state.ticketDetails) return state

        const toolCall = await executeToolCall('createTicket', {
            title: state.ticketDetails.title,
            description: state.ticketDetails.description,
            priority: state.ticketDetails.priority
        }, state.userId)

        if (!toolCall.error) {
            const ticketResult = toolCall.result as { id: string, title: string }
            return {
                ...state,
                toolCalls: [...state.toolCalls, toolCall],
                response: `I've created a support ticket for you: /tickets/${ticketResult.id} - "${ticketResult.title}". A support representative will assist you shortly.`
            }
        }

        return state
    },

    // Generate response
    generate_response: async (state: ConversationState) => {
        if (state.response) return state // Skip if we already have a response

        const messages = [
            new SystemMessage(SYSTEM_PROMPT),
            ...state.messages.map(msg =>
                msg.role === 'user'
                    ? new HumanMessage(msg.content)
                    : new SystemMessage(msg.content)
            ),
            new HumanMessage(
                state.context.length
                    ? `Context from knowledge base:\n${formatContext(state.context)}\n\nUser message: ${state.currentMessage}`
                    : `No relevant context found in knowledge base. Current features I can help with:\n` +
                    `â€¢ Customer support and ticket management\n` +
                    `â€¢ Knowledge base access and information\n` +
                    `â€¢ General system navigation and usage\n\n` +
                    `User message: ${state.currentMessage}`
            )
        ]

        const response = await model.invoke(messages)
        let responseText = response.content.toString()

        // Make responses more conversational
        if (responseText.length > 500 && !responseText.includes('â€¢')) {
            responseText = responseText.slice(0, 500) + "... I can provide more specific details about any of these points - what would you like to know more about?"
        } else if (responseText.length > 800) {
            responseText = responseText.slice(0, 800) + "... I can elaborate on any of these points - just let me know what interests you most!"
        }

        return {
            ...state,
            response: responseText,
            metrics: {
                ...state.metrics,
                rgqs: {
                    response_text: responseText,
                    overall_quality: 0.9,
                    relevance: state.context.length ? 0.95 : 0.7,
                    accuracy: state.context.length ? 0.95 : 0.7,
                    tone: 0.9
                }
            }
        }
    },

    // Record metrics
    record_metrics: async (state: ConversationState) => {
        const traceId = Math.random().toString(36).substring(7)
        if (state.metrics.kra) {
            await recordKRAMetrics(traceId, state.ticketId, state.metrics.kra)
        }
        if (state.metrics.rgqs) {
            await recordRGQSMetrics(traceId, state.ticketId, state.metrics.rgqs)
        }
        return state
    }
}

// Create our conversation pipeline
const pipeline = RunnableSequence.from([
    async (state: ConversationState) => await nodes.analyze_intent(state),
    async (state: ConversationState) => await nodes.gather_context(state),
    async (state: ConversationState) => await nodes.analyze_ticket_need(state),
    async (state: ConversationState) => await nodes.handle_ticket(state),
    async (state: ConversationState) => await nodes.generate_response(state),
    async (state: ConversationState) => await nodes.record_metrics(state)
])

export async function processConversation(
    message: string,
    userId: string,
    previousMessages: ChatMessage[],
    ticketId: string | null = null
): Promise<AgentResponse> {
    try {
        // Initialize state
        const initialState: ConversationState = {
            messages: previousMessages,
            context: [],
            userId,
            ticketId,
            currentMessage: message,
            needsTicket: false,
            toolCalls: [],
            response: null,
            metrics: {},
            isFirstMessage: previousMessages.length === 0
        }

        // Run the pipeline
        const result = await pipeline.invoke(initialState)

        return {
            message: result.response || 'I apologize, but I encountered an error processing your request.',
            tool_calls: result.toolCalls,
            context_used: result.context,
            metrics: result.metrics
        }
    } catch (error) {
        console.error('Error in conversation processing:', error)
        return {
            message: 'I apologize, but I encountered an error processing your request. Please try again or contact human support.',
            metrics: {}
        }
    }
} 